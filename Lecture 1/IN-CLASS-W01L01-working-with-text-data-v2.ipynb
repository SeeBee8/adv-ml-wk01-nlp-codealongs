{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34a8583-3d57-4f07-8a10-9459ee3cdb09",
   "metadata": {},
   "source": [
    "Advanced ML Week 1, Lecture 1: Working with and Preparing Text Data\n",
    "\n",
    "In this notebook we will be preparing Twitter (X) Tweets for sentiment analysis.  Sentiment analysis is a common text classification challenge to determine whether a text is positive or negative.  \n",
    "\n",
    "This is useful for companies that want to analyze large numbers of documents, tweets, reviews, etc., to determine public sentiment about a product or service.\n",
    "\n",
    "The data was originally gathered from Twitter (now X) and hand-labeled.  Of course there will be some human bias in the labeling.  It was downloaded from Kaggle at this site: [Kaggle Twitter Tweets Sentiment Dataset](https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset/)\n",
    "\n",
    "There are 3 classes: positive, negative, and neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c695c-5cfd-4069-aacf-dacd0457e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0645a-3d8a-41f9-a678-23b5862ac8a2",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "We will download our **corpus** of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f666473-51b7-4c60-8ce6-ae94e5b3d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download corpus of tweets\n",
    "df = pd.read_csv('Data/archive.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15206203-a97b-448d-b5e9-a95c511ec56a",
   "metadata": {},
   "source": [
    "# Some light EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e60c31-28a8-48a2-a150-cc2eb9ed3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e96d1-975b-493b-9551-9e242871df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafde953-8792-47b9-861e-3f53b6e3a177",
   "metadata": {},
   "source": [
    "# Some Light Data Cleaning\n",
    "\n",
    "We see that our **corpus** has 27481 **documents**, each with an ID, the full text, a shortened version, and the labeled sentiment.\n",
    "\n",
    "Interestingly, one of the tweets has no text!  We definitely want to get rid of that.  We will also drop the `textID` and `selected_text` columns.  We are going to use the entire text of each tweet, not just a subset.\n",
    "\n",
    "We will keep the label, `sentiment` for later classification and analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be5747-3e29-4745-ab38-c25e09255dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['textID', 'selected_text'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcd7d-1e2f-4d44-bb98-43ece33d3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b390c-c90c-46d4-be9f-061e8a66f2f2",
   "metadata": {},
   "source": [
    "# Some More EDA\n",
    "Let's look at some aspects of this text.\n",
    "* What do the **documents** look like?\n",
    "* How long do the tend to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656c034-5ce8-4214-8147-a18f1150699f",
   "metadata": {},
   "source": [
    "## View some sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e8add-e130-49ac-8ffa-9dcb7d59568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand how many characters pandas will show\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "## Display some of the documents (tweets)\n",
    "df['text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935409bd-a7fb-4100-b043-59846d065328",
   "metadata": {},
   "source": [
    "We can see here that there are some URLs in the text.  This will be a problem for normalization.  We will remove those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cd2bd-5fbf-4d38-b932-05ae2de1e25e",
   "metadata": {},
   "source": [
    "## Get some statistics on the length of **documents**\n",
    "\n",
    "Let's see how long each tweet is and determine the average length of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65b4f6-78ec-4344-aca0-83c43a78336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the length of each tweet\n",
    "## Create a new column of the lengths of each tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62170df2-9dda-479d-b003-37a401889bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the statistics of the lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dafca-dc80-4cba-862d-e9ba98de19dd",
   "metadata": {},
   "source": [
    "The tweets have an mean length of 68 characters and a median of 64. They range from 3 to 141 characters with a standard deviation of 35.  The middle 50% are between 39 and 97 characters in length.\n",
    "\n",
    "This gives us some idea of how long they tend to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f7818-5a2e-48f2-87bf-29678b7f9be6",
   "metadata": {},
   "source": [
    "# Text Normalization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a869f6-073e-4281-b21a-33aba2fdd553",
   "metadata": {},
   "source": [
    "## Normalizing Casing\n",
    "\n",
    "It's common practice to lower the casing of the text in our documents to contribut to normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550db04b-32a7-4270-9420-e6da4eaadb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lower the casing of each document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174777b-b52a-4260-b486-b75ec1a2ba0f",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizing text into single word tokens is simple in Python.  We can just use `str.split()`.  The default separator for `.split()` is one space, so `' '`.\n",
    "\n",
    "We can access Pandas' string accessor with `df.str.<method>`.  This allows us to apply string methods to all rows in a column.\n",
    "\n",
    "When processing text, if memory allows, it can be useful to keep many versions of your text: tokenize, lemmatized, no stop words, etc.  Some analysis or modeling packages expect tokenized data and others do not.  We often want to use different versions for different kinds of analysis, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b244d34-3f00-4b0a-af13-d9f0ee324398",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the documents into tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68391d27-b08b-469a-b5cc-c2571b849b14",
   "metadata": {},
   "source": [
    "### Better way to tokenize data\n",
    "\n",
    "NLTK has a more sophisticated tokenization function that will isolate things like punctuation as well.  This way 'hooray' and 'hooray!!!' will be the same token.\n",
    "\n",
    "In order for NLTK to recognize the punctuation, we will need to download the 'punkt' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb8036-ffbe-4365-aacc-3ee5cc5e05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download punkt\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Tokenize with nltk.word_tokenize instead\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f435f-e144-4b72-8c07-3249dae5b164",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91158-047d-4ba7-9a6b-67ae623a33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download NLTK stopword list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Load the English stop words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bd5a-fe7d-4766-baec-55ceb0f2f78c",
   "metadata": {},
   "source": [
    "<font color=red> NOTICE </font> that all of the stop words are lower case.  It's necessary to ensure that your tokens are all lower case before using this list to remove stop words.\n",
    "\n",
    "To remove the stop words from each document, we will apply a function that will check each word in the list of tokens against the list of stopwords and remove them if they are in the list.  More specifically, it will only save them if they are NOT in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cd190-07ee-40ee-b6c2-4c8639ff4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function to remove stop words\n",
    "\n",
    "## Apply the function to the tokenized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e8234-98a9-4e5b-8994-71be25afc15d",
   "metadata": {},
   "source": [
    "## Remove Punctuation\n",
    "\n",
    "We can remove punctuation in a similar that we removed stop words.  However, we will get our list of punctuation from the built in Python string library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb2857-8b5f-4c61-99f0-3a4151f0c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import built-in String Libary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02a29f-39f3-4671-93f8-0452c223fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function to remove punctuation tokens\n",
    "\n",
    "\n",
    "\n",
    "## Apply the function to the tokens without punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4d7d9-b014-493c-90f3-26af7bbbc93a",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f40a1-9c75-4aca-be8e-b14d370a302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove URLs\n",
    "## Create function to remove URL tokens\n",
    "\n",
    "\n",
    "\n",
    "## Apply the function to the tokens without URLs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac0458-1764-4f82-ba99-0dd7ee47a16e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Note how many fewer tokens we have in our `no_stops_no_punct` tokens than in our original.  However, some information was lost, but a lot was also retained.  \n",
    "\n",
    "Normalization is a huge part of the NLP process and is always a balance between reducing the size of our vocabulary and therefor simplifying our models, and retaining enough information for the model to extract some meaningful patterns in the texts.  \n",
    "\n",
    "There are a lot of choices here to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed88e6-d9b1-4a54-943c-76d26209f01b",
   "metadata": {},
   "source": [
    "# Normalizing Text with spaCy\n",
    "\n",
    "The spaCy Python package provides text processing pipelines that can do many of these operations, plus much more complicated processing, very fast and in many fewer steps.  For this reason it is a very popular tool.  \n",
    "\n",
    "It utilizes pretrained language models that can recognize things like parts of speech and named entities (people, specific places, currency, etc.)\n",
    "\n",
    "spaCy was not included in your original dojo_env, so you will need to install if if you have not already.\n",
    "\n",
    "We will also download the pretrained english language model trained on millions of web documents.  We will use the small sized one for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e87dc-befc-4cc1-b970-75fcac2026fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install spacy if necessary\n",
    "!pip install spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "## Download the English small-sized model trained on web documents if necessary\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a50f2",
   "metadata": {},
   "source": [
    "## The spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416eaea3-add2-481f-b77e-79c67901f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model.  Disable Named Entity Recognizer (too slow)\n",
    "nlp_model = spacy.load('en_core_web_sm', disable='ner')\n",
    "\n",
    "## Display the names of each tranformer pipe\n",
    "nlp_model.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec96eaf",
   "metadata": {},
   "source": [
    "We have our model, and we can apply it like a function.  It expects a string of text as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process a document with the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a8134",
   "metadata": {},
   "source": [
    "The document is a collection of tokens we can iterate over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca267ae",
   "metadata": {},
   "source": [
    "## Documents and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the tokens in the document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69343c",
   "metadata": {},
   "source": [
    "Each token is much more than a string.  It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b189b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolate the last token in the document\n",
    "\n",
    "\n",
    "## Display the text and type of the token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad5b18",
   "metadata": {},
   "source": [
    "Each has many attributes that we can take advantage of, such as the lemma form and whether it is punctuation or space, and whether it is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the lemmatized form of the token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2938ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether the token is punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether the token is a space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ab785",
   "metadata": {},
   "source": [
    "Spacy can even determine the part of speech that the token is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c38600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the part of speech of the token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the parts of speech for each token in the document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of the lemmas for each token in the document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06915d48",
   "metadata": {},
   "source": [
    "Notice that spaCy does not lower the case of lemmas.  Let's make sure we do that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa373c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of only the tokens in the document that are not punctuation or spaces or URLs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of all the tokens in the document that are not punctuation, spaces, or stop words\n",
    "[token.lemma_.lower() for token in doc if \n",
    " not token.is_punct and \n",
    " not token.is_space and \n",
    " not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5eaac",
   "metadata": {},
   "source": [
    "In order to use spaCy to process our entire dataframe, we will need to make a function and apply it to our text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8936b-44b5-4b5b-a332-1fc90ba7c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's also remove URLs\n",
    "## Let's also remove the url\n",
    "[token.lemma_.lower() for token in doc if \n",
    " not token.is_punct and \n",
    " not token.is_space and \n",
    " not token.is_stop and \n",
    " not 'http' in token.lemma_.lower() and\n",
    " not 'www' in token.lemma_.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437115ec",
   "metadata": {},
   "source": [
    "## Preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to use spacy to process our text\n",
    "\n",
    "\n",
    "\n",
    "## Process the tweets using the spaCy function into a new column in the df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce1f3",
   "metadata": {},
   "source": [
    "We used spaCy to tokenize, lemmatize, and remove punctuation and stopwords from our text in one step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f35e6",
   "metadata": {},
   "source": [
    "Notice that the spaCy processed data is a little different than our previously processed data.  The text has been lemmatized and spaCy has a different list of stop words than NLTK.\n",
    "\n",
    "The learn platform has directions for how you can customize your spaCy stopword list and a function with more flexibility in how spaCy will process your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6f5c2",
   "metadata": {},
   "source": [
    "# ngrams\n",
    "combine multiple words into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0413ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the ngrams function\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Isolate the 6th lemmatized document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create list of bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5764e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create list of trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddca69",
   "metadata": {},
   "source": [
    "\n",
    "## Applying `ngrams` to make a new column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715de044",
   "metadata": {},
   "source": [
    "\n",
    "We need to make a function that returns a list of bigrams.  It won't work to just pass the ngrams function to `.apply()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4604ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create a function to create bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bigrams to the df with .apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e8efd",
   "metadata": {},
   "source": [
    "\n",
    "# Save the final data version for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Save the processed data\n",
    "# df.to_csv('../Data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144062b1-ad45-4ac8-b587-d26a0758d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the processed data\n",
    "# import joblib\n",
    "\n",
    "# joblib.dump(df, '../Data/processed_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875936d-5cf9-453d-b530-d173bcd2e03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
